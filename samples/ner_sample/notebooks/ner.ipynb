{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition using Flair on CONLL-2003\n",
    "## Experiment description\n",
    "This notebook contains a ML fabric flow for Named Entity Recognition using the [flair NLP package](https://github.com/flairNLP/flair/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jupyter helpers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import flair\n",
    "from flair.datasets import CONLL_03\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, PooledFlairEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "import torch\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.data import DataLoader\n",
    "from src.models import BaseModel\n",
    "from src.data_processing import DataProcessor, EmptyProcessor\n",
    "from src.experimentation import MlflowExperimentation\n",
    "from src.evaluation import Evaluator, EvaluationMetrics\n",
    "from src import ExperimentRunner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConllDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, dataset_name, dataset_version='1', local_data_path = '../data/processed/'):\n",
    "        self.folds = ('eng.train', 'eng.testa', 'eng.testb')\n",
    "        self.local_data_path = local_data_path\n",
    "        super().__init__(dataset_name=dataset_name, dataset_version=dataset_version)\n",
    "\n",
    "    def download_dataset(self) -> None:\n",
    "        \n",
    "        if self.dataset_name==\"conll_03\" and self.dataset_version == '1':\n",
    "            \n",
    "            for fold in self.folds:\n",
    "                local_path = Path(self.local_data_path,self.dataset_name).resolve()\n",
    "                \n",
    "                if not local_path.exists():\n",
    "                    local_path.mkdir(parents=True)\n",
    "\n",
    "                dataset_file = Path(local_path, fold)\n",
    "                if dataset_file.exists():\n",
    "                    print(\"Dataset already exists, skipping download\")\n",
    "                    return\n",
    "\n",
    "                dataset_path=f\"https://raw.githubusercontent.com/glample/tagger/master/dataset/{fold}\"\n",
    "                response = requests.get(dataset_path)\n",
    "                dataset_raw = response.text\n",
    "                with open(dataset_file, \"w\") as f:\n",
    "                    f.write(dataset_raw)\n",
    "                print(f\"Finished writing fold {fold} to {self.local_data_path}\")\n",
    "\n",
    "            print(f\"Finished downloading dataset {self.dataset_name} version {self.dataset_version}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Selected dataset was not found\")\n",
    "\n",
    "    def get_dataset(self) -> Corpus:\n",
    "        try:\n",
    "            return CONLL_03(base_path=self.local_data_path, in_memory=True)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Dataset {self.dataset_name} with version {self.dataset_version} not found in data/raw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "*replace MyDataLoader with your DataLoader implementation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download\n",
      "2020-06-04 11:59:05,577 Reading data from ..\\data\\processed\\conll_03\n",
      "2020-06-04 11:59:05,580 Train: ..\\data\\processed\\conll_03\\eng.train\n",
      "2020-06-04 11:59:05,581 Dev: ..\\data\\processed\\conll_03\\eng.testa\n",
      "2020-06-04 11:59:05,582 Test: ..\\data\\processed\\conll_03\\eng.testb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flair.datasets.sequence_labeling.CONLL_03 at 0x25a85ae2d08>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = ConllDataLoader(dataset_name = \"conll_03\")\n",
    "data_loader.download_dataset()\n",
    "corpus = data_loader.get_dataset()\n",
    "corpus=corpus.downsample(0.05) # Just for example purposes\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample in train sample:\n",
      " Sentence: \"EU rejects German call to boycott British lamb .\"   [− Tokens: 9  − Token-Labels: \"EU <NNP/I-NP/S-ORG> rejects <VBZ/I-VP> German <JJ/I-NP/S-MISC> call <NN/I-NP> to <TO/I-VP> boycott <VB/I-VP> British <JJ/I-NP/S-MISC> lamb <NN/I-NP> . <.>\"]\n"
     ]
    }
   ],
   "source": [
    "print(f\"First sample in train sample:\\n {corpus.train.dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Define experimentation object, which will be used for logging the experiments parameters, metrics and artifacts\n",
    "*Replace MlflowExperimentation if you use a different experimentation system*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentation = MlflowExperimentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Create model/logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-04 11:59:15,116 | INFO : loading Word2VecKeyedVectors object from C:\\Users\\ommendel\\.flair\\embeddings\\glove.gensim\n",
      "2020-06-04 11:59:16,625 | INFO : loading vectors from C:\\Users\\ommendel\\.flair\\embeddings\\glove.gensim.vectors.npy with mmap=None\n",
      "2020-06-04 11:59:16,802 | INFO : setting ignored attribute vectors_norm to None\n",
      "2020-06-04 11:59:16,803 | INFO : loaded C:\\Users\\ommendel\\.flair\\embeddings\\glove.gensim\n",
      "2020-06-04 11:59:18,669 | INFO : Created model FlairNERModel with hyperparams {'hidden_size': 256, 'pooling': 'min', 'word_embeddings': 'glove', 'train_with_dev': True, 'max_epochs': 10, 'training': True, 'use_rnn': True, 'use_crf': False, 'rnn_layers': 1, 'trained_epochs': 0, 'tag_type': 'ner', 'tagset_size': 20, 'beta': 1.0, 'nlayers': 1, 'use_dropout': 0.0, 'use_word_dropout': 0.05, 'use_locked_dropout': 0.5, 'pickle_module': 'pickle', 'reproject_to': 8292, 'relearn_embeddings': True, 'train_initial_hidden_state': False, 'bidirectional': True, 'rnn_type': 'LSTM'}\n"
     ]
    }
   ],
   "source": [
    "flair.device = torch.device(\"cpu\")\n",
    "\n",
    "class FlairNERModel(BaseModel):\n",
    "\n",
    "    def __init__(self, \n",
    "                 corpus: Corpus,\n",
    "                 hidden_size: int =256, \n",
    "                 pooling: str = 'min', \n",
    "                 word_embeddings: str='glove',\n",
    "                 train_with_dev: bool = True,\n",
    "                 max_epochs: int = 10):\n",
    "        self.tag_type = 'ner'\n",
    "        self.tag_dictionary = None\n",
    "        self.tagger = None\n",
    "        self.embeddings = None\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pooling = pooling\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.train_with_dev = train_with_dev\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.set_tagger_definition(corpus)\n",
    "        \n",
    "        hyper_params = self.get_hyper_params(hidden_size=hidden_size, \n",
    "                             pooling=pooling, \n",
    "                             word_embeddings=word_embeddings, \n",
    "                             train_with_dev=train_with_dev, \n",
    "                             max_epochs=max_epochs)\n",
    "        \n",
    "        super().__init__(**hyper_params)\n",
    "\n",
    "    def get_hyper_params(self, **hyper_params):\n",
    "        basic_params = {param_name: param_value \n",
    "                        for (param_name, param_value) in self.tagger.__dict__.items() \n",
    "                        if type(param_value) in (bool, float, int, str)}\n",
    "        hyper_params.update(basic_params)\n",
    "        return hyper_params\n",
    "        \n",
    "    def set_embeddings_definition(self) -> List[TokenEmbeddings]:\n",
    "        \"\"\"\n",
    "        Sets the embedding layers used by this tagger\n",
    "        \"\"\"\n",
    "        # initialize embeddings\n",
    "        embedding_types: List[TokenEmbeddings] = [\n",
    "\n",
    "        # Word embeddings (default = GloVe)\n",
    "        WordEmbeddings(self.word_embeddings),\n",
    "\n",
    "        # contextual string embeddings, forward\n",
    "        PooledFlairEmbeddings('news-forward', pooling=self.pooling),\n",
    "\n",
    "        # contextual string embeddings, backward\n",
    "        PooledFlairEmbeddings('news-backward', pooling=self.pooling)\n",
    "        ]\n",
    "        self.embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "    \n",
    "    def set_tagger_definition(self, corpus:Corpus) -> SequenceTagger:\n",
    "        \"\"\"\n",
    "        Returns the definition of the Flair SequenceTagger (the full model)\n",
    "        :param corpus: Used only for setting the tag_dictionary\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.embeddings:\n",
    "            self.set_embeddings_definition()\n",
    "        self.tag_dictionary = corpus.make_tag_dictionary(tag_type=self.tag_type)\n",
    "        \n",
    "        tagger: SequenceTagger = SequenceTagger(hidden_size=self.hidden_size,\n",
    "                                                embeddings=self.embeddings,\n",
    "                                                tag_dictionary=self.tag_dictionary,\n",
    "                                                tag_type=self.tag_type,\n",
    "                                                use_crf=False)\n",
    "        self.tagger = tagger\n",
    "        \n",
    "    def fit(self, corpus: Corpus) -> None:\n",
    "        # initialize trainer\n",
    "        trainer: ModelTrainer = ModelTrainer(self.tagger, corpus)\n",
    "\n",
    "        trainer.train('models/taggers/flair-ner',\n",
    "                      train_with_dev=self.train_with_dev,  \n",
    "                      max_epochs=self.max_epochs)\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        tagged_sentences = []\n",
    "        for sentence in tqdm(sentences):\n",
    "            self.tagger.predict(sentence)\n",
    "            tagged_sentences.append(sentence)\n",
    "        return tagged_sentences\n",
    "            \n",
    "model = FlairNERModel(corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-04 11:59:18,682 loading file C:\\Users\\ommendel\\.flair\\models\\en-ner-conll03-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "TRAIN=False\n",
    "\n",
    "if TRAIN:\n",
    "    model.fit(corpus)\n",
    "else:\n",
    "    # Simulate training has finished by downloading a pretrained model\n",
    "    model.tagger = SequenceTagger.load('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dff9e0fd14a4270a9901fcc7ee47636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " In | O (1.0)\n",
      " Penny | B-LOC (0.9802)\n",
      " Lane, | E-LOC (0.999)\n",
      " there | O (1.0)\n",
      " is | O (1.0)\n",
      " a | O (1.0)\n",
      " barber | O (1.0)\n",
      " showing | O (1.0)\n",
      " photographs | O (1.0)\n"
     ]
    }
   ],
   "source": [
    "example_sentence = Sentence(\"In Penny Lane, there is a barber showing photographs\")\n",
    "\n",
    "model.predict([example_sentence])\n",
    "for token in example_sentence.tokens:\n",
    "    #print(token.__dict__)\n",
    "    print(f\" {token.text} | {token.get_tag('ner')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O (1.0)\n",
      "O (1.0)\n",
      "O (1.0)\n",
      "O (1.0)\n",
      "O (1.0)\n",
      "O (1.0)\n"
     ]
    }
   ],
   "source": [
    "for sentence in [corpus.test[i] for i in range(5)]:\n",
    "    [token.add_tag_label(\"gold_ner\",token.get_tag(\"ner\")) for token in sentence.tokens]\n",
    "    [token.set_label(\"ner\",value=\"O\") for token in sentence.tokens]\n",
    "#for token in test1:\n",
    "    \n",
    "\n",
    "for token in corpus.test[1]:\n",
    "    print(token.get_tag(\"ner\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x25ab881f1c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in [corpus.test[i] for i in range(5)]:\n",
    "    [token.add_tag_label(\"gold_ner\",token.get_tag(\"ner\")) for token in sentence.tokens]\n",
    "    [token.set_label(\"ner\",value=\"O\") for token in sentence.tokens]\n",
    "\n",
    "predictions = model.predict(corpus.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Define evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class NEREvaluationMetrics:\n",
    "    \"\"\"\n",
    "    This class holds the metrics calculated during the experiment run\n",
    "    \"\"\"\n",
    "    def __init__(self,f1, accuracy):\n",
    "        self.f1 = f1\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "class NEREvaluator(Evaluator):\n",
    "    \"\"\"\n",
    "    This class holds the logic for evaluating a prediction outcome\n",
    "    \"\"\"\n",
    "    def evaluate(self, y_test, predicted_sentences) -> EvaluationMetrics:\n",
    "        golds = []\n",
    "        predicted = []\n",
    "        print(y_test)\n",
    "        print(predicted_sentences)\n",
    "        for sentence in predicted_sentences:\n",
    "\n",
    "            gold_tags = [token.get_tag('gold_ner').value for token in sentence.tokens]\n",
    "            golds.append(gold_tags)\n",
    "            predicted_tags = [token.get_tag('ner').value for token in sentence.tokens]\n",
    "            predicted.append(predicted_tags)\n",
    "        \n",
    "        f1 = f1_score(golds, predicted)\n",
    "        accuracy = accuracy_score(golds, predicted)\n",
    "        return NEREvaluationMetrics(f1=f1,accuracy=accuracy)\n",
    "        \n",
    "evaluator = NEREvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define experiment runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-04 15:15:41,999 | INFO : Starting experiment: Experiment...\n",
      "2020-06-04 15:15:42,000 | INFO : Connecting to MlflowExperimentation\n",
      "Logging package in C:\\Users\\ommendel\\OneDrive - Microsoft\\Projects\\MLExperimentationFramework\\samples\\ner_sample\\src\n",
      "None\n",
      "[]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-fa59a6fdcb7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment_runner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ommendel\\onedrive - microsoft\\projects\\mlexperimentationframework\\samples\\ner_sample\\src\\experiment_runner.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mevaluation_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_experiment\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluation_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStepEvaluationMetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-424eb314308f>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, y_test, predicted_sentences)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgolds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgolds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mNEREvaluationMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ner_sample\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[0mnb_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_correct\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnb_true\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "class NERExperimentRunner(ExperimentRunner):\n",
    "    \n",
    "    def __init__(self,\n",
    "                model,\n",
    "                corpus,\n",
    "                data_loader,\n",
    "                log_experiment,\n",
    "                experiment_logger,\n",
    "                evaluator,\n",
    "                experiment_name):\n",
    "        self.corpus = corpus\n",
    "        super().__init__(model=model,\n",
    "                         data_loader=data_loader, \n",
    "                         log_experiment=log_experiment,\n",
    "                         experiment_logger=experiment_logger,\n",
    "                         evaluator=evaluator, \n",
    "                         experiment_name=experiment_name,\n",
    "                         X_train=None, X_test=None)\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Overwriting the ExperimentRunner predict to verify that tags are marked correctly before prediction\n",
    "        \"\"\"\n",
    "        corpus = self.corpus\n",
    "        # Copy gold NER to new label and assign O to all ner labels (to be populated during inference)\n",
    "        for sentence in corpus.test:\n",
    "            [token.add_tag_label(\"gold_ner\",token.get_tag(\"ner\")) for token in sentence.tokens]\n",
    "            [token.set_label(\"ner\",value=\"O\") for token in sentence.tokens]\n",
    "\n",
    "        self._predictions = self.model.predict(corpus.test)\n",
    "\n",
    "\n",
    "experiment_runner = NERExperimentRunner(\n",
    "    model=model,\n",
    "    corpus=corpus,\n",
    "    data_loader=data_loader,\n",
    "    log_experiment=True,\n",
    "    experiment_logger=experimentation,\n",
    "    evaluator=evaluator,\n",
    "    experiment_name=\"Experiment\",\n",
    ")\n",
    "\n",
    "results = experiment_runner.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('ner_sample': conda)",
   "language": "python",
   "name": "python37664bitnersampleconda9d5f3ecca728487ca0fbc34bec2b6e6f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
